{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import neologdn\n",
    "import time\n",
    "import os\n",
    "import urllib.request\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import MeCab\n",
    "from __future__ import unicode_literals\n",
    "import re\n",
    "import unicodedata\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import boto3\n",
    "import json\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "import codecs\n",
    "plt.style.use('fivethirtyeight')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "#chaneg dummies to matrix\n",
    "from scipy.sparse import csr_matrix,coo_matrix\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras import initializers\n",
    "from keras.engine.topology import Layer\n",
    "from keras.layers import Dense, Input\n",
    "from keras.layers import Embedding, GRU, Bidirectional, TimeDistributed,CuDNNGRU,CuDNNLSTM\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from nltk import tokenize\n",
    "import os\n",
    "import tarfile\n",
    "import numpy as np\n",
    "\n",
    "import string\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import gensim\n",
    "path=\"/home/ec2-user/SageMaker/Notebooks_For_CX_Usecases/Multiple_Labels_Text_Classfication/notebook/models/word2vec-models/word2vec_100d_mfreq5.model\"\n",
    "word_vectors = gensim.models.Word2Vec.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize japanase words\n",
    "def wakati_by_mecab(text):\n",
    "    tagger = MeCab.Tagger('-Ochasen -d /usr/lib64/mecab/dic/mecab-ipadic-neologd')\n",
    "    tagger.parse('') \n",
    "    node = tagger.parseToNode(text)\n",
    "    word_list = []\n",
    "    while node:\n",
    "        pos = node.feature.split(\",\")[0]\n",
    "        #if pos in [\"名詞\",\"動詞\",\"形容詞\",\"記号\"]:   # 対象とする品詞\n",
    "        if pos in [\"名詞\"]:   # 対象とする品詞\n",
    "            word = node.surface\n",
    "            word_list.append(word)\n",
    "        node = node.next\n",
    "    return \" \".join(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_chars(row):\n",
    "    cust_list=['\\d+','■','\\n','・','◇','①','②','③','④','【】','】','【','⇒','●','★',':','※','→']\n",
    "    del_list = string.ascii_letters + string.punctuation\n",
    "    for i in del_list :\n",
    "        row = row.str.replace(i,'')\n",
    "    for i in cust_list :\n",
    "        row = row.str.replace(i,'')\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path=\"/home/ec2-user/SageMaker/Notebooks_For_CX_Usecases/Multiple_Labels_Text_Classfication/raw_data/raw_claim_data.xlsx\"\n",
    "df = pd.read_excel(file_path, sheetname=None, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sheet_all = pd.concat(df.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sheet_all.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=['協会分類原因'\n",
    "     ,'苦情の要約（100文字以内程度）','申出内容詳細1（500文字以内程度）','申出内容詳細2（500文字以内程度）']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cols=['協会分類原因','協会分類（大）','協会分類（中）','協会分類（小Ⅰ）','協会分類（小Ⅱ）'\n",
    "#     ,'苦情の要約（100文字以内程度）']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_col=df_sheet_all[cols]\n",
    "df_col.columns = ['kyokai_reason','cmt0','cmt1','cmt2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_col=df_sheet_all[cols]\n",
    "#df_col.columns = ['kyokai_reason','kyokai_big','kyokai_mid','kyokai_big_small_1', 'kyokai_big_small_2','cmt0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#削除いらない分類\n",
    "#indexNames = df_col[ (df_col['kyokai_big_small_1'] == \"022 営業店からの請求書類送付遅い\") | (df_col['kyokai_big_small_1'] == \"書類到着確認フォロー\") ].index\n",
    "#df_col.drop(indexNames, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace old with new values\n",
    "#df_col.replace({'kyokai_big_small_2': {\"012 担当者訪問なし\": \"012 アフターフォロー不足\", \n",
    "#                                       \"013 保全等手続き･説明不十分\": \"013 説明不十分、対応不満\",\n",
    "#                                       \"034 保険の仕組（例、掛捨、満期）\":\"保険の仕組（例、掛捨、満期、変額）\"}},inplace=True)\n",
    "\n",
    "#df_col.replace({'claim_B': {\"001 ａ：通知類が見づらい\": \"001 ａ：通知類圧着不具合\", \n",
    "#                                       \"003 ｃ：通知類の情報量が足りない\": \"003 ｃ：通知類が分かりにくい、情報が足りない\",\n",
    "#                                       \"007 ｇ：対応不十分（CSC）\":\"007 ｇ：対応不十分（CSC）・オペリスク\"}},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_col.fillna(\"\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_num(s):\n",
    "    return re.sub('[^0-9]+', '', s) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only keep numbers in category\n",
    "for col in ['kyokai_reason']:\n",
    "    df_col[col]=df_col[col].apply(remove_non_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_tra=df_col.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.get_dummies(df_col, drop_first=True, prefix=['kyokai_reason'], prefix_sep='-')\n",
    "\n",
    "cols_to_transform=['kyokai_reason']\n",
    "df_tra=pd.get_dummies(df_col,columns=cols_to_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine text\n",
    "cols = ['cmt0','cmt1','cmt2']\n",
    "df_tra['comment'] =df_tra[cols].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "df_tra.drop(columns=cols,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tra['comment'] = remove_chars(df_tra['comment'])\n",
    "df_tra['comment']=df_tra['comment'].apply(neologdn.normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_sent(comment):\n",
    "    sent_detector = nltk.RegexpTokenizer(u'[^　！？。]*[！？。.\\n]')\n",
    "    sents = sent_detector.tokenize(comment)\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 100\n",
    "max_sentences = 20\n",
    "max_words = 20000\n",
    "embedding_dim = 300\n",
    "validation_split = 0.3\n",
    "reviews = []\n",
    "labels = []\n",
    "texts = []\n",
    "glove_dir = \"./glove.6B\"\n",
    "embeddings_index = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for idx in range(df_tra.comment.shape[0]):\n",
    "    text = df_tra.comment[idx]\n",
    "    texts.append(text)\n",
    "    sentences = return_sent(text)\n",
    "    reviews.append(sentences)\n",
    "\n",
    "labels=df_tra[df_tra.columns[:-1]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = Pool(processes=multiprocessing.cpu_count())\n",
    "df_tra['comment'] = pool.map(wakati_by_mecab,df_tra['comment'])\n",
    "pool.close() \n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text=list(df_tra['comment'].values)\n",
    "tokenizer = Tokenizer(num_words=max_words,lower=False)\n",
    "tokenizer.fit_on_texts(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data = np.zeros((len(texts), max_sentences, maxlen), dtype='int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for i, sentences in enumerate(reviews):\n",
    "    for j, sent in enumerate(sentences):\n",
    "        if j < max_sentences:\n",
    "            wordTokens = wakati_by_mecab(sent).split()\n",
    "            k = 0\n",
    "            for _, word in enumerate(wordTokens):\n",
    "                try:\n",
    "                    if k < maxlen and tokenizer.word_index[word] < max_words:\n",
    "                        data[i, j, k] = tokenizer.word_index[word]\n",
    "                        k = k + 1\n",
    "                except:\n",
    "                    data[i, j, k]=20001\n",
    "                    k = k + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with open('sent_word_arr.pkl','wb') as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sent_word_arr.pkl','rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Total %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape of reviews (data) tensor:', data.shape)\n",
    "print('Shape of sentiment (label) tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(validation_split * data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of each category in training and validation set')\n",
    "print(y_train.sum(axis=0))\n",
    "print(y_val.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(path):\n",
    "    with open(path,'rb') as f:\n",
    "        emb_arr = pickle.load(f)\n",
    "    return emb_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time.time\n",
    "fasttext_embeddings=load_embeddings('/home/ec2-user/SageMaker/word2vector/fasttext_300d.pkl')\n",
    "print('loaded '+ str(len(fasttext_embeddings)) +' word vectors in {time.time()-tic}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total %s word vectors.' % len(fasttext_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building Hierachical Attention network\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    try:\n",
    "        embedding_vector = fasttext_embeddings[word]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "    except:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        continue\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import initializers as initializers, regularizers, constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionWithContext(Layer):\n",
    "    \"\"\"\n",
    "    Attention operation, with a context/query vector, for temporal data.\n",
    "    Supports Masking.\n",
    "    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
    "    \"Hierarchical Attention Networks for Document Classification\"\n",
    "    by using a context vector to assist the attention\n",
    "    # Input shape\n",
    "        3D tensor with shape: `(samples, steps, features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(samples, features)`.\n",
    "    How to use:\n",
    "    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "    The dimensions are inferred based on the output shape of the RNN.\n",
    "    Note: The layer has been tested with Keras 2.0.6\n",
    "    Example:\n",
    "        model.add(LSTM(64, return_sequences=True))\n",
    "        model.add(AttentionWithContext())\n",
    "        # next add a Dense layer (for classification/regression) or whatever...\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "\n",
    "        self.u = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_u'.format(self.name),\n",
    "                                 regularizer=self.u_regularizer,\n",
    "                                 constraint=self.u_constraint)\n",
    "\n",
    "        super(AttentionWithContext, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        uit = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "\n",
    "        uit = K.tanh(uit)\n",
    "        ait = dot_product(uit, self.u)\n",
    "\n",
    "        a = K.exp(ait)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(word_index) + 1, embedding_dim, weights=[embedding_matrix],\n",
    "                            input_length=maxlen, trainable=True, mask_zero=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_input = Input(shape=(maxlen,), dtype='int32')\n",
    "char_sequences = embedding_layer(char_input)\n",
    "char_lstm = Bidirectional(GRU(100, return_sequences=True))(char_sequences)\n",
    "char_dense = TimeDistributed(Dense(200))(char_lstm)\n",
    "char_att = AttentionWithContext()(char_dense)\n",
    "charEncoder = Model(char_input, char_att)\n",
    "\n",
    "words_input = Input(shape=(max_sentences, maxlen), dtype='int32')\n",
    "words_encoder = TimeDistributed(charEncoder)(words_input)\n",
    "words_lstm = Bidirectional(GRU(100, return_sequences=True))(words_encoder)\n",
    "words_dense = TimeDistributed(Dense(200))(words_lstm)\n",
    "words_att = AttentionWithContext()(words_dense)\n",
    "preds = Dense(5, activation='softmax')(words_att)\n",
    "model = Model(words_input, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RocAuc = RocAucEvaluation(validation_data=(x_val, y_val), interval=1)\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "          epochs=5, batch_size=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_pickle('processed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tra['comment']=df['comment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tra['comment'] = remove_chars(df_tra['comment'])\n",
    "df_tra['comment']=df_tra['comment'].apply(neologdn.normalize)\n",
    "pool = Pool(processes=multiprocessing.cpu_count())\n",
    "df_tra['comment'] = pool.map(wakati_by_mecab,df_tra['comment'])\n",
    "pool.close() \n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tra['comment'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_len(cmt):\n",
    "    return len(cmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_tra['comment'].apply(return_len).sort_values(ascending=False)\n",
    "#df_tra['len']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#train, test = train_test_split(df_mutilabel, test_size=0.2)\n",
    "\n",
    "train_texts = df_tra[\"comment\"].values\n",
    "train_labels = df_tra[df_tra.columns[:-1]].values\n",
    "\n",
    "#test_texts = test[\"comment\"].values\n",
    "#test_labels = test[['cat_brochure', 'cat_compass', 'cat_myaxa', 'cat_reputaion', 'cat_keiyaku','cat_tetuduki','cat_tanto','cat_other']].values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "text=list(df_tra['comment'].values)\n",
    "\n",
    "\n",
    "top_vocab=10000\n",
    "\n",
    "tokenizer = Tokenizer(top_vocab)\n",
    "#list xは上に書いたようにテキストデータが格納されている\n",
    "tokenizer.fit_on_texts(text)\n",
    "\n",
    "maxlen_value=1000\n",
    "\n",
    "# 对每一句影评文字转换为数字列表，使用每个词的编号进行编号\n",
    "x_train_seq = tokenizer.texts_to_sequences(train_texts)\n",
    "#x_test_seq = tokenizer.texts_to_sequences(test_texts)\n",
    "x_train = sequence.pad_sequences(x_train_seq, maxlen=maxlen_value)\n",
    "#x_test = sequence.pad_sequences(x_test_seq, maxlen=maxlen_value)\n",
    "y_train = np.array(train_labels)\n",
    "#y_test = np.array(test_labels)\n",
    "with open('tokenizer/kyokai_reason_ML_TXT_Claim_tokenizer_BiLSTM_BiGRU.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "x = list(tokenizer.word_counts.items())\n",
    "s = sorted(x,key=lambda p:p[1],reverse=True)\n",
    "word_index = tokenizer.word_index # 不受vocab_size的影响\n",
    "small_word_index = copy.deepcopy(word_index) # 防止原来的字典也被改变了\n",
    "print(\"Removing less freq words from word-index dict...\")\n",
    "for item in s[top_vocab:]:\n",
    "    small_word_index.pop(item[0])\n",
    "print(\"Finished!\")\n",
    "print(len(small_word_index))\n",
    "print(len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "#配布されていたfastTextの次元数は300だったので300\n",
    "EMBEDDING_DIM = 100\n",
    "#embedding_metrix = np.zeros(top_vocab+1,EMBEDDING_DIM)\n",
    "embedding_matrix = np.random.uniform(size=(top_vocab+1,EMBEDDING_DIM))\n",
    "#word_vectors = gensim.models.KeyedVectors.load_word2vec_format('/home/ec2-user/SageMaker/entitiy_vector_dic/entity_vector/entity_vector.model.bin', binary=True)\n",
    "for word, i in small_word_index.items():\n",
    "    try:\n",
    "        #print (word)\n",
    "        embedding_vector = word_vectors.wv[word]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    except KeyError:\n",
    "        print(\"Word: [\",word,\"] not in wvmodel! Use random embedding instead.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_multi_label_metric(y_true, y_pred):\n",
    "    comp = K.equal(y_true, K.round(y_pred))\n",
    "    return K.cast(K.all(comp, axis=-1), K.floatx())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Input, concatenate, Flatten, SpatialDropout1D\n",
    "from keras.layers import CuDNNLSTM,LSTM, Bidirectional, GlobalMaxPool1D, Dropout, CuDNNGRU,GRU, GlobalAveragePooling1D, GlobalMaxPooling1D,Conv1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard, LearningRateScheduler, Callback\n",
    "from keras.optimizers import Adam, Adadelta, SGD, RMSprop, Nadam\n",
    "\n",
    "def get_model():\n",
    "    inputs = Input(shape=(maxlen_value,))\n",
    "    class_count = 5\n",
    "    wv_dim=100\n",
    "    vocab_size=top_vocab\n",
    "    use_pretrained_wv = True\n",
    "    if use_pretrained_wv:\n",
    "        wv = Embedding(vocab_size+1,wv_dim,input_length=maxlen_value,weights=[embedding_matrix],trainable=True)(inputs)\n",
    "    else:\n",
    "        wv = Embedding(vocab_size+1,wv_dim,input_length=MAXLEN)(inputs)\n",
    "        \n",
    "    #x = SpatialDropout1D(0.2)(wv)\n",
    "    x = Bidirectional(CuDNNGRU(256, return_sequences=True))(wv)\n",
    "    x = Bidirectional(CuDNNGRU(256, return_sequences=True))(x)\n",
    "    x = Conv1D(64,8,padding='same', activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    x = concatenate([avg_pool, max_pool])\n",
    "    x = Dense(128, activation=\"relu\")(x)\n",
    "    x = Dense(class_count, activation=\"softmax\")(x)\n",
    "    model = Model(inputs=inputs, outputs=x)\n",
    "    model.summary()\n",
    "    opt = Adam(lr=1e-3)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"models/kyoka_reason_ML_TXT_Claim_LSTM-GRU-Stack-weights-improvement-{val_acc:.4f}.hdf5\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    " \n",
    "cp_callback = ModelCheckpoint(checkpoint_path,\n",
    "                              save_best_only=True,\n",
    "                              verbose=0)\n",
    " \n",
    "callbacks = [\n",
    "    EarlyStopping(patience=150, monitor='val_acc',mode='max'),\n",
    "    cp_callback\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 10\n",
    "m = get_model()\n",
    "\n",
    "m.fit(x_train,y_train,batch_size=batch_size,epochs=epochs,shuffle=True,validation_split=0.2,callbacks=callbacks,verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
